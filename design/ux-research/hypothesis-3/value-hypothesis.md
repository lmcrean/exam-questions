# Value Hypothesis: All-in-One Flexible Assessment Platform

## Core Value Propositions

### For Teachers (Primary Driver)

**Unified Workflow:**
- **One tool for all assessment types:** Daily starters → weekly exam practice → termly data drops
- **No context-switching:** Same login, same interface, same workflow
- **Reduce tool fatigue:** Replace 3 tools (Mentimeter + Google Classroom + manual marking) with one
- **Cognitive load reduction:** Learn one platform, not three

**Flexibility & Dynamic Decision-Making:**
- **Decide *after* submission:** Students submit → teacher decides: grade? display? peer review?
- **Turn starters into assessments:** "That starter was great, let's grade it retroactively"
- **No rigid planning:** Don't need to decide upfront if something is "graded" or not
- **Reuse content:** Same question can be starter today, graded assessment tomorrow
- **Seamless escalation:** Formative practice → summative assessment without re-setup

**Better Than Mentimeter (For Starters):**
- **No character limits:** Handle exam-style 500+ word responses (not just short polls)
- **Pattern-spotting:** AI analyzes all 30 responses, highlights common misconceptions
- **Long-form support:** Code examples, diagrams, detailed explanations
- **Persistent responses:** Saved for review, grading, or future reference (not ephemeral)
- **Marking integration:** Can instantly add rubric and grade if desired

**Better Than Manual Marking (For Data Drops):**
- **80% time savings:** 55 hours → <10 hours for 330 students
- **AI-assisted marking:** Consistent rubric application, instant WWW/EBI feedback
- **Faster feedback loop:** Students receive feedback in minutes (not 1-2 weeks)
- **Pattern analysis:** AI heatmaps show class-wide gaps before individual review
- **Reduced burnout:** Less repetitive manual work

### For Students (Secondary Driver)

**Continuity & Progress:**
- **One platform for all work:** Starter practice → exam practice → coursework (same UX)
- **See progress over time:** Formative feedback informs summative improvement
- **No fragmentation:** Don't lose work across multiple platforms

**Instant Feedback:**
- **Immediate WWW/EBI:** AI feedback within minutes of submission
- **Learn while fresh:** Mistakes corrected same day (not 2 weeks later)
- **Faster iteration:** Revise and resubmit based on instant feedback

**Engagement & Peer Learning:**
- **See peer responses:** Live display for starters, optional for assessments
- **Vote on strong examples:** Identify quality work through peer review
- **Multiple approaches:** Learn from seeing how 30 classmates answered

---

## Hypothesis Statement

**"If teachers had one flexible platform that seamlessly handled starters (better than Mentimeter) and data drop marking (80% time savings with AI), they would use it daily for formative assessment and termly for summative assessment, replacing 3 separate tools."**

**Key Assumptions to Test:**
1. Teachers value "one tool for everything" over specialized tools
2. Flexibility to "decide after submission" is a killer feature (not just nice-to-have)
3. Can be "better than Mentimeter" for starters (no character limits, better pattern-spotting)
4. Can be "better than manual marking" for data drops (80% time savings, 85% accuracy)
5. AI accuracy satisfies both low-stakes (starters) and high-stakes (data drops)
6. Daily formative use drives retention between termly data drop cycles
7. "Replacing 3 tools" is a strong enough value prop to overcome switching costs

---

## Competitive Positioning

### vs. Mentimeter (Starters Only)

**Mentimeter's limitations:**
- ❌ Character limits (200-500 chars) → can't do exam-style questions
- ❌ No marking integration → can't grade responses
- ❌ Short-lived → responses disappear after session
- ❌ No AI analysis → teacher manually reads all responses

**This product's advantages:**
- ✅ **No character limits** → full exam-style 500+ word responses
- ✅ **Marking integration** → can grade any starter retroactively
- ✅ **Persistent** → all responses saved, can review later
- ✅ **AI pattern-spotting** → instant heatmap of common mistakes
- ✅ **Continuity** → starters can become graded assessments

**Why teachers would switch:**
"Mentimeter is great for quick polls, but I need a tool that handles *real* exam questions and can grade them if I want."

---

### vs. Manual Marking (Data Drops Only)

**Manual marking's limitations:**
- ❌ 55+ hours for 330 students
- ❌ Evening/weekend work
- ❌ Feedback delayed 1-2 weeks
- ❌ Consistency issues
- ❌ No engagement features

**This product's advantages:**
- ✅ **80% time savings** → AI marks, teacher reviews in <10 hours
- ✅ **Work-life balance** → no evening/weekend marathon marking sessions
- ✅ **Instant feedback** → students get WWW/EBI in minutes
- ✅ **Consistency** → AI applies rubric uniformly
- ✅ **Engagement** → optional peer review, live display

**Why teachers would switch:**
"I need to mark 330 students this term. If AI can save me 45+ hours, I'm in."

---

### vs. Using Multiple Tools (Status Quo)

**Current fragmented workflow:**
- Mentimeter for starters (5-10 min)
- Paper for exam practice (20-30 min)
- Google Classroom for submissions
- Manual marking (55+ hours)
- Google Classroom for feedback delivery

**Pain points of fragmentation:**
- ❌ **Context-switching:** 3+ logins, 3+ interfaces to learn
- ❌ **Data silos:** Starter responses in Mentimeter, coursework in GC, feedback in notebooks
- ❌ **Cognitive load:** Remember which tool for which task
- ❌ **Time waste:** Set up Mentimeter, then set up GC, then manual marking workflow
- ❌ **No continuity:** Can't reuse starter questions for graded assessments
- ❌ **No flexibility:** Must decide upfront "is this graded?" before creating assignment

**This product's advantages:**
- ✅ **One login, one interface** → reduce cognitive load
- ✅ **All data in one place** → starters + assessments + grades + feedback
- ✅ **One workflow to learn** → applies to all assessment types
- ✅ **Flexible decision-making** → decide to grade *after* seeing responses
- ✅ **Reusable content** → starter question → graded assessment (one click)

**Why teachers would switch:**
"I'm tired of juggling Mentimeter + Google Classroom + manual marking. One tool that does everything would be a relief."

---

## Value Prop by User Segment

### Segment 1: "The Overwhelmed Marker" (Primary Target)

**Profile:**
- Teaches 90-330 students (multiple KS3/KS4 classes)
- Data drop marking is biggest time drain (55+ hours/term)
- Uses Mentimeter occasionally but limited by character counts
- Desperately needs time savings for work-life balance

**Primary pain:** Marking workload (Hypothesis 2)
**Secondary pain:** Starters are dry (Hypothesis 1)

**Value prop:**
"**Save 45+ hours per data drop with AI marking—and get a better Mentimeter for daily starters too.**"

**Adoption driver:** Time savings (H2) → stay for engagement features (H1)

---

### Segment 2: "The Engagement Enthusiast" (Secondary Target)

**Profile:**
- Loves using Mentimeter/Kahoot for starters
- Frustrated by character limits for exam-style questions
- Wants better pattern-spotting across responses
- Marking is a pain but not the primary concern

**Primary pain:** Mentimeter limitations (Hypothesis 1)
**Secondary pain:** Marking workload (Hypothesis 2)

**Value prop:**
"**Mentimeter for exam questions—with no character limits, AI pattern-spotting, and optional grading.**"

**Adoption driver:** Better starters (H1) → discover marking features later (H2)

---

### Segment 3: "The Tool Minimalist" (Tertiary Target)

**Profile:**
- Frustrated by fragmented tools (Mentimeter + GC + marking)
- Values simplicity and unified workflows
- Wants one platform to rule them all

**Primary pain:** Tool fragmentation (Hypothesis 3 unique)
**Secondary pain:** Both marking and engagement

**Value prop:**
"**One tool for all assessment—from starters to data drops. No more context-switching.**"

**Adoption driver:** "One tool" philosophy (H3 unique)

---

## Success Metrics (How We Know It's Working)

### Adoption Metrics

**Cross-workflow usage (Critical for H3):**
- ✅ 70%+ of teachers use for BOTH starters AND marking (not just one workflow)
- ✅ Average teacher creates 10+ assignments per term (mix of formative + summative)
- ✅ 50%+ report "replaced Mentimeter" (better for starters)
- ✅ 50%+ report "replaced manual marking" (better for data drops)

**If only one workflow is used:**
- ⚠️ If only used for starters → just Hypothesis 1, not Hypothesis 3
- ⚠️ If only used for marking → just Hypothesis 2, not Hypothesis 3

---

### Flexibility Metrics (Unique to H3)

**Dynamic decision-making:**
- ✅ 20%+ of assignments use "decide to grade after submission" feature
- ✅ 30%+ of teachers report "flexibility to grade retroactively is valuable"
- ✅ 10%+ of starter questions escalated to graded assessments

**If flexibility is unused:**
- ⚠️ Feature doesn't add value → simplify to two separate workflows

---

### Time Savings (From H2)

**Marking efficiency:**
- ✅ 80%+ time savings for data drops (55 hours → <10 hours)
- ✅ 85%+ AI accuracy (grade alignment with teacher judgement)
- ✅ Teachers submit AI-assisted grades to leadership (trust validation)

---

### Engagement (From H1)

**Better than Mentimeter:**
- ✅ 70%+ say "better than Mentimeter for exam questions"
- ✅ Used weekly (or more) for starters
- ✅ Students ask "can we do this again?"

---

### Consolidation (H3 Unique)

**Tool replacement:**
- ✅ 60%+ stop using Mentimeter (replaced)
- ✅ 60%+ stop manual marking workflow (replaced)
- ✅ 50%+ report "reduced number of tools I use"

**If tools aren't replaced:**
- ⚠️ Not compelling enough vs. specialized tools → rethink value prop

---

## Wizard of Oz Testing Plan

### Test 1: Starters (Validate "Better than Mentimeter")

**Setup:**
- Use Wooclap for exam-style questions (500+ words, no limits)
- Display all responses on board
- Enable voting/peer review
- Optional: Simulate AI pattern-spotting (manual analysis)

**Questions:**
- Is this better than Mentimeter for exam practice?
- Do you miss any Mentimeter features?
- Would you switch from Mentimeter to this?

**Success criteria:**
- ✅ "Yes, I'd switch from Mentimeter" (70%+ of testers)
- ✅ "No character limit is a big deal" (unprompted feedback)

---

### Test 2: Marking (Validate AI Time Savings)

**Setup:**
- Students submit coursework via Google Forms
- Manually mark with rubric (simulate AI consistency)
- Provide instant WWW/EBI feedback
- Track time: marking + review vs. manual baseline

**Questions:**
- Is marking time reduced by 80%?
- Do you trust AI grades for data drops?
- Would you submit these grades to leadership?

**Success criteria:**
- ✅ 80%+ time savings achieved
- ✅ 85%+ grade accuracy (spot-check)
- ✅ "I'd trust these grades" (2+ teachers)

---

### Test 3: Flexibility (Validate "Decide Later" Workflow) — UNIQUE TO H3

**Setup:**
- Run a starter session (don't mention grading)
- After submission, ask: "Do you want to grade this?"
- If yes, add rubric retroactively and mark
- Track: Do teachers use this? Is it valuable?

**Questions:**
- Did you know you could grade this when you started?
- Would you use "decide later" grading regularly?
- Does this flexibility change how you plan lessons?

**Success criteria:**
- ✅ 30%+ of teachers say "I'd use this feature"
- ✅ At least one teacher says "This changes my workflow"

**If flexibility is not valued:**
- ⚠️ Drop "decide later" feature → build separate workflows (H1 + H2)

---

### Test 4: "One Tool" Value Prop (Validate Consolidation) — UNIQUE TO H3

**Method: Teacher Interviews**

**Questions:**
- How many tools do you use for assessment? (Mentimeter, GC, etc.)
- Is managing multiple tools a pain point?
- Would you prefer one tool for all assessment types?
- What's the biggest downside of switching from specialized tools?

**Listen for:**
- "I wish I had one login for everything" → Strong H3 signal
- "I like specialized tools for specific tasks" → Weak H3 signal

**Success criteria:**
- ✅ 60%+ prefer "one tool" over specialized tools
- ✅ "Managing multiple tools is a pain" (unprompted)

**If teachers prefer specialized tools:**
- ⚠️ Pivot to H1 or H2 (don't force all-in-one)

---

## Decision Tree (After Testing)

### Scenario A: All Three Validated ✅

**Evidence:**
- Better than Mentimeter for starters (Test 1)
- 80% time savings on marking (Test 2)
- Flexibility is valued (Test 3)
- "One tool" resonates (Test 4)

**→ Build Hypothesis 3 (All-in-One Platform)**
- Unified codebase, two UI modes (formative vs. summative)
- Flexible workflow (decide after submission)
- Position as "one tool for all assessment"

---

### Scenario B: Starters + Marking Validated, Flexibility Not Valued ⚠️

**Evidence:**
- Better than Mentimeter (Test 1)
- AI marking saves time (Test 2)
- But "decide later" is unused (Test 3)
- "One tool" is nice but not compelling (Test 4)

**→ Build Hybrid Platform (H1 + H2 Separate Workflows)**
- Two distinct modes: "Quick Check" (H1) vs. "Assignment" (H2)
- Teacher chooses upfront (no post-submission flexibility)
- Position as "two tools in one" (not "one flexible tool")

---

### Scenario C: Only Marking Validated (H2 Wins) ⚠️

**Evidence:**
- Mentimeter replacement is weak (Test 1)
- But AI marking is strong (Test 2)
- Flexibility not valued (Test 3)
- "One tool" doesn't resonate (Test 4)

**→ Build Hypothesis 2 Only (AI Marking Assistant)**
- Focus on data drops, skip starters
- Position as "AI marking assistant"
- Maybe add H1 features later (if demand)

---

### Scenario D: Only Starters Validated (H1 Wins) ⚠️

**Evidence:**
- Better than Mentimeter (Test 1)
- But AI marking doesn't work well enough (Test 2)
- Flexibility not valued (Test 3)

**→ Build Hypothesis 1 Only (Better Mentimeter)**
- Focus on formative starters, skip marking
- Position as "Mentimeter for exam questions"
- Maybe add AI marking later (if improves)

---

### Scenario E: "One Tool" Not Valued (Kill H3) ⚠️

**Evidence:**
- Test 4 shows teachers prefer specialized tools
- "Managing multiple tools" is not a pain point

**→ Pivot to H1 or H2 (Don't Force All-in-One)**
- Build specialized tool (better Mentimeter OR marking assistant)
- Don't try to be everything

---

## Open Questions to Resolve

1. **Is "decide after submission" a killer feature or just nice-to-have?**
   - Test: Run starter, ask retroactively "want to grade this?"
   - Listen: Do teachers light up? Or shrug?

2. **Do teachers value "one tool" enough to switch from specialized tools?**
   - Test: Interview 10 teachers about tool fragmentation pain
   - Listen: "I hate managing multiple tools" vs. "I like specialized tools"

3. **Can AI handle both low-stakes (starters) and high-stakes (data drops)?**
   - Test: AI marks both short responses (starters) and long coursework
   - Measure: Accuracy on both types (does one suffer?)

4. **Is "better than Mentimeter" a strong enough hook?**
   - Test: Teachers try it for starters
   - Ask: "Would you switch from Mentimeter?"
   - Listen: Enthusiastic yes vs. polite maybe

5. **Does daily use (starters) drive retention for termly use (data drops)?**
   - Hypothesis: Teachers who use it weekly for starters will remember it for data drops
   - Test: Track usage patterns (weekly → termly correlation?)

6. **Would teachers pay more for "all-in-one" vs. specialized tools?**
   - Test: Pricing research (willingness to pay)
   - Question: "Would you pay $X/month for one tool that does both?"

---

## Iteration Roadmap

### Phase 0: Validate All Three Hypotheses (2-4 weeks)

**Run all four tests:**
1. Mentimeter replacement (starters)
2. AI marking (data drops)
3. Flexibility ("decide later" workflow)
4. "One tool" value prop (interviews)

**Decision point:**
- If all validated → Build Hypothesis 3
- If some validated → Build Hybrid (H1+H2) or specialize (H1 or H2)

---

### Phase 1: Core Platform (4-6 weeks)

**Shared foundation:**
1. Student submission (text + file upload)
2. Mark scheme / rubric builder
3. AI analysis engine (grading + feedback)
4. Assignment creator

**Essential for both workflows**

---

### Phase 2: Dual Workflows (3-4 weeks)

**Workflow A: Formative (Starters)**
5. Live display mode (all responses on board)
6. Voting / peer review
7. Optional grading toggle

**Workflow B: Summative (Marking)**
8. Teacher review interface (bulk grade adjustment)
9. Grade export (CSV)
10. Instant student feedback delivery

---

### Phase 3: Flexibility Features (2-3 weeks) — IF VALIDATED

**Dynamic decision-making:**
11. "Decide to grade after submission" toggle
12. Escalate starter → assessment (one click)
13. Reuse questions across contexts

**Skip if flexibility not valued in tests**

---

## Pricing Strategy

### Free Tier: "Try Both Workflows"
- 30 submissions/month
- Both formative (starters) and summative (marking) workflows
- AI feedback (limited)
- Basic analytics

**Goal:** Let teachers experience both workflows before committing

---

### Premium Tier: "Unlimited Assessment"
- Unlimited submissions
- AI marking for data drops (high volume)
- Advanced analytics (heatmaps, class patterns)
- Priority support
- MIS integration (SIMS, Arbor)

**Pricing:** $10-15/month per teacher OR $200-300/year school license

**Value prop:** "Replace 3 tools (Mentimeter + GC + marking time) for less than a Mentimeter Pro subscription"

---

## Competitive Advantages (Summary)

**vs. Mentimeter:**
- ✅ No character limits
- ✅ Exam-style long-form responses
- ✅ Can grade responses
- ✅ AI pattern-spotting

**vs. Manual Marking:**
- ✅ 80% time savings
- ✅ Instant feedback
- ✅ Consistent grading
- ✅ Class analytics

**vs. Fragmented Workflows:**
- ✅ One tool, one login
- ✅ All data in one place
- ✅ Flexible decision-making
- ✅ Continuity (formative → summative)

**Tagline:**
"**The only assessment tool you need—from starters to data drops.**"

---

## Critical Success Factor for Hypothesis 3

**The "all-in-one" platform only succeeds if:**

1. ✅ Used for BOTH starters AND marking (not just one)
2. ✅ Better than Mentimeter for starters
3. ✅ Better than manual marking for data drops
4. ✅ "One tool" reduces cognitive load (teachers feel relief, not overwhelm)
5. ✅ Flexibility is actually used (not ignored)

**If only used for one workflow:**
- ⚠️ It's not Hypothesis 3—it's H1 or H2 in disguise

**If flexibility is ignored:**
- ⚠️ Just build two separate workflows (don't force dynamic features)

**Test these assumptions before committing to H3 build.**
